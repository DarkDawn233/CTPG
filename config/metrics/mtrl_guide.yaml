# @package _group_
train:
  [
      # The format is [key, abbr, type, average/constant]
      # If a key ends with "_", n (= number of envs),
      # copies of the metric are created.
    [episode, E, int, average],
    [step, S, int, average],
    [duration, D, time, average],
    [episode_reward, R, float, average],
    [success, Su, float, average],
    [batch_reward, BR, float, average],
    [actor_loss, ALOSS, float, average],
    [critic_loss, CLOSS, float, average],
    [guide_actor_loss, GALOSS, float, average],
    [guide_critic_loss, GCLOSS, float, average],
    [guide_critic_comp_loss, GCCLOSS, float, average],
    [actor_mask, AMASK, float, average],
    [critic_mask, CMASK, float, average],
    [guide_actor_mask, GAMASK, float, average],
    [guide_critic_mask, GCMASK, float, average],
    [guide_critic_comp_mask, GCCMASK, float, average],
    [ae_loss, RLOSS, float, average],
    [ae_transition_loss, null, float, average],
    [reward_loss, null, float, average],
    [actor_target_entropy, null, float, average],
    [actor_entropy, null, float, average],
    [guide_actor_target_entropy, null, float, average],
    [guide_actor_entropy, null, float, average],
    [alpha_loss, null, float, average],
    [alpha_, null, float, average],
    [alpha_value, null, float, average],
    [guide_alpha_loss, null, float, average],
    [guide_alpha_, null, float, average],
    [guide_alpha_value, null, float, average],
    [contrastive_loss, MLOSS, float, average],
    [max_rat, MR, float, average],
    [env_index, ENV, str, constant],
    [reward_, R_, float, average],
    [success_, Su_, float, average],
    [env_step_, ES_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, null, float, average],
    [critic_loss_agent_index_, AGENT_, float, average],
    [actor_distilled_agent_loss_agent_index_, null, float, average],
    [actor_loss_agent_index_, null, float, average],
    [actor_target_entropy_agent_index_, null, float, average],
    [actor_entropy_agent_index_, null, float, average],
    [alpha_loss_agent_index_, null, float, average],
    [alpha_value_agent_index_, null, float, average],
    [ae_loss_agent_index_, null, float, average],
  ]
eval:
  [
    [episode, E, int, average],
    [step, S, int, average],
    [episode_reward, R, float, average],
    [env_index, ENV, str, constant],
    [success, Su, float, average],
    [episode_reward_, R_, float, average],
    [success_, Su_, float, average],
    [env_step_, ES_, float, average],
    # [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, AGENT_, float, average],
  ]
